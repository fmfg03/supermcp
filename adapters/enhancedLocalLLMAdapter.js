/**
 * Enhanced Local LLM Adapter with Langwatch Integration
 * 
 * Adaptador universal para modelos LLM locales (Mistral, LLaMA, DeepSeek)
 * con integraci√≥n completa de Langwatch para tracking de m√©tricas,
 * scores simulados y contradicci√≥n expl√≠cita
 * 
 * @module enhancedLocalLLMAdapter
 */

import { spawn } from 'child_process';
import path from 'path';
import fs from 'fs';
import logger from '../backend/src/utils/logger.js';
import { LocalLLMWrapperFactory, withLocalLangwatch } from '../backend/src/services/localLLMWrappers.js';
import { enhancedLangwatchMiddleware } from '../backend/src/middleware/enhancedLangwatchMiddleware.js';
import { contradictionService, detectContradiction, generateContradictionPrompt } from '../backend/src/services/contradictionService.js';

/**
 * Configuraci√≥n de modelos locales con Langwatch
 */
const ENHANCED_LOCAL_MODELS_CONFIG = {
  'mistral-local': {
    scriptPath: 'scripts/run_local_mistral.py',
    modelPath: 'models/mistral.gguf',
    displayName: 'Mistral Local',
    maxTokens: 512,
    temperature: 0.7,
    description: 'Modelo Mistral local optimizado para tareas generales y razonamiento',
    langwatch: {
      enabled: true,
      modelName: 'mistral-local',
      provider: 'local',
      tags: ['local', 'mistral', 'general', 'reasoning']
    }
  },
  'llama-local': {
    scriptPath: 'scripts/run_local_llama.py',
    modelPath: 'models/llama.gguf',
    displayName: 'LLaMA Local',
    maxTokens: 512,
    temperature: 0.7,
    description: 'Modelo LLaMA local potente para comprensi√≥n y generaci√≥n de texto',
    langwatch: {
      enabled: true,
      modelName: 'llama-local',
      provider: 'local',
      tags: ['local', 'llama', 'comprehension', 'generation']
    }
  },
  'deepseek-local': {
    scriptPath: 'scripts/run_local_deepseek.py',
    modelPath: 'models/deepseek.gguf',
    displayName: 'DeepSeek Local',
    maxTokens: 512,
    temperature: 0.3,
    description: 'Modelo DeepSeek local especializado en matem√°ticas y l√≥gica compleja',
    langwatch: {
      enabled: true,
      modelName: 'deepseek-local',
      provider: 'local',
      tags: ['local', 'deepseek', 'mathematics', 'logic']
    }
  }
};

/**
 * Configuraci√≥n por defecto para fallbacks
 */
const DEFAULT_CONFIG = {
  maxTokens: 512,
  temperature: 0.7,
  timeout: 30000, // 30 segundos
  encoding: 'utf-8',
  langwatch: {
    enabled: true,
    trackContradiction: true,
    trackMetrics: true,
    simulateScores: true
  }
};

/**
 * Clase principal del adaptador de LLMs locales con Langwatch
 */
export class EnhancedLocalLLMAdapter {
  constructor() {
    this.modelsConfig = ENHANCED_LOCAL_MODELS_CONFIG;
    this.projectRoot = process.cwd();
    this.isInitialized = false;
    this.langwatchWrappers = {};
    this.sessionHistory = new Map();
  }
  
  /**
   * Inicializa el adaptador con Langwatch
   */
  async initialize() {
    try {
      logger.info('üîß Inicializando Enhanced LocalLLMAdapter con Langwatch...');
      
      // Verificar directorios
      const requiredDirs = ['scripts', 'models'];
      for (const dir of requiredDirs) {
        const dirPath = path.join(this.projectRoot, dir);
        if (!fs.existsSync(dirPath)) {
          fs.mkdirSync(dirPath, { recursive: true });
          logger.info(`üìÅ Directorio creado: ${dir}`);
        }
      }
      
      // Inicializar wrappers de Langwatch
      await this.initializeLangwatchWrappers();
      
      // Verificar disponibilidad de modelos
      const availableModels = await this.checkAvailableModels();
      logger.info('üìä Modelos disponibles:', availableModels);
      
      this.isInitialized = true;
      logger.info('‚úÖ Enhanced LocalLLMAdapter inicializado exitosamente');
      
      return {
        success: true,
        availableModels,
        totalModels: Object.keys(this.modelsConfig).length,
        langwatchEnabled: true
      };
      
    } catch (error) {
      logger.error('‚ùå Error inicializando Enhanced LocalLLMAdapter:', error);
      throw error;
    }
  }
  
  /**
   * Inicializa wrappers de Langwatch para cada modelo
   */
  async initializeLangwatchWrappers() {
    try {
      for (const modelType of Object.keys(this.modelsConfig)) {
        this.langwatchWrappers[modelType] = LocalLLMWrapperFactory.createWrapper(modelType);
        logger.info(`üîç Wrapper Langwatch inicializado para ${modelType}`);
      }
    } catch (error) {
      logger.error('Error inicializando wrappers Langwatch:', error);
      throw error;
    }
  }
  
  /**
   * Llama a un modelo local con tracking completo de Langwatch
   */
  async callLocalModelWithLangwatch(modelType, prompt, options = {}) {
    try {
      if (!this.isInitialized) {
        await this.initialize();
      }
      
      const startTime = Date.now();
      const sessionId = options.sessionId || `enhanced_${Date.now()}`;
      
      logger.info(`ü§ñ Llamando modelo local con Langwatch: ${modelType}`);
      
      // Validar modelo
      const modelConfig = this.modelsConfig[modelType];
      if (!modelConfig) {
        throw new Error(`Modelo no soportado: ${modelType}`);
      }
      
      // Verificar disponibilidad
      const availability = await this.checkModelAvailability(modelType);
      if (!availability.available) {
        throw new Error(`Modelo no disponible: ${availability.reason}`);
      }
      
      // Obtener historial de la sesi√≥n
      const sessionHistory = this.getSessionHistory(sessionId);
      const currentAttempt = sessionHistory.length + 1;
      
      // Detectar si aplicar contradicci√≥n expl√≠cita
      const contradictionAnalysis = detectContradiction(
        sessionId,
        currentAttempt,
        sessionHistory
      );
      
      let finalPrompt = prompt;
      let contradictionApplied = false;
      
      // Aplicar contradicci√≥n si es necesario
      if (contradictionAnalysis.shouldApply) {
        const contradictionPrompt = generateContradictionPrompt(
          prompt,
          modelType,
          contradictionAnalysis.intensity,
          sessionHistory
        );
        
        finalPrompt = contradictionPrompt.prompt;
        contradictionApplied = true;
        
        logger.info(`üî• Contradicci√≥n ${contradictionAnalysis.intensity} aplicada para ${modelType}`);
      }
      
      // Preparar par√°metros para el modelo
      const params = {
        prompt: finalPrompt,
        maxTokens: options.maxTokens || modelConfig.maxTokens || DEFAULT_CONFIG.maxTokens,
        temperature: options.temperature || modelConfig.temperature || DEFAULT_CONFIG.temperature,
        sessionId,
        modelPath: path.join(this.projectRoot, modelConfig.modelPath),
        contradictionApplied,
        attemptNumber: currentAttempt,
        ...options
      };
      
      // Funci√≥n para ejecutar el modelo
      const executeModel = async (prompt, opts) => {
        return await this.executePythonScript(modelConfig.scriptPath, {
          ...params,
          prompt
        });
      };
      
      // Ejecutar con wrapper de Langwatch
      const wrapper = this.langwatchWrappers[modelType];
      const result = await wrapper.withLangwatch(executeModel, finalPrompt, {
        sessionId,
        modelType,
        previousAttempts: sessionHistory,
        contradictionApplied,
        contradictionIntensity: contradictionAnalysis.intensity,
        tags: [...(modelConfig.langwatch.tags || []), ...(options.tags || [])]
      });
      
      const duration = Date.now() - startTime;
      
      // Procesar respuesta con metadata completa
      const enhancedResponse = {
        success: true,
        modelType,
        displayName: modelConfig.displayName,
        response: result.response,
        tokenUsage: result.tokenUsage,
        metadata: {
          ...result.metadata,
          duration,
          sessionId,
          attemptNumber: currentAttempt,
          contradictionApplied,
          contradictionAnalysis: contradictionApplied ? contradictionAnalysis : null,
          langwatchTracking: result.langwatch
        }
      };
      
      // Evaluar efectividad de contradicci√≥n si se aplic√≥
      if (contradictionApplied && sessionHistory.length > 0) {
        const previousScore = sessionHistory[sessionHistory.length - 1]?.langwatch?.score || 0;
        const effectivenessAnalysis = contradictionService.evaluateContradictionEffectiveness(
          sessionId,
          {
            intensity: contradictionAnalysis.intensity,
            previousScore
          },
          {
            score: result.langwatch.score
          }
        );
        
        enhancedResponse.metadata.contradictionEffectiveness = effectivenessAnalysis;
      }
      
      // Guardar en historial de sesi√≥n
      this.saveToSessionHistory(sessionId, enhancedResponse);
      
      logger.info(`‚úÖ Modelo ${modelType} respondi√≥ con Langwatch en ${duration}ms - Score: ${result.langwatch.score?.toFixed(3)}`);
      
      return enhancedResponse;
      
    } catch (error) {
      logger.error(`‚ùå Error en modelo ${modelType} con Langwatch:`, error);
      
      // Tracking de error en Langwatch
      await enhancedLangwatchMiddleware.trackLocalModelError({
        sessionId: options.sessionId || 'unknown',
        modelName: modelType,
        prompt,
        error: error.message,
        duration: Date.now() - (options.startTime || Date.now())
      });
      
      return {
        success: false,
        modelType,
        error: error.message,
        fallbackUsed: false,
        metadata: {
          duration: Date.now() - (options.startTime || Date.now()),
          timestamp: new Date().toISOString(),
          langwatchTracked: true
        }
      };
    }
  }
  
  /**
   * Verifica disponibilidad de modelos
   */
  async checkAvailableModels() {
    const available = {};
    
    for (const [modelType, config] of Object.entries(this.modelsConfig)) {
      const modelPath = path.join(this.projectRoot, config.modelPath);
      const scriptPath = path.join(this.projectRoot, config.scriptPath);
      
      available[modelType] = {
        modelExists: fs.existsSync(modelPath),
        scriptExists: fs.existsSync(scriptPath),
        modelPath: config.modelPath,
        scriptPath: config.scriptPath,
        displayName: config.displayName,
        description: config.description,
        langwatchEnabled: config.langwatch.enabled
      };
    }
    
    return available;
  }
  
  /**
   * Verifica si un modelo espec√≠fico est√° disponible
   */
  async checkModelAvailability(modelType) {
    const config = this.modelsConfig[modelType];
    if (!config) {
      return { available: false, reason: 'Modelo no configurado' };
    }
    
    const modelPath = path.join(this.projectRoot, config.modelPath);
    const scriptPath = path.join(this.projectRoot, config.scriptPath);
    
    if (!fs.existsSync(scriptPath)) {
      return { available: false, reason: 'Script Python no encontrado' };
    }
    
    if (!fs.existsSync(modelPath)) {
      return { available: false, reason: 'Archivo de modelo .gguf no encontrado' };
    }
    
    return { available: true, reason: 'Modelo disponible' };
  }
  
  /**
   * Ejecuta un script Python con par√°metros
   */
  async executePythonScript(scriptPath, params) {
    return new Promise((resolve, reject) => {
      const fullScriptPath = path.join(this.projectRoot, scriptPath);
      
      // Preparar argumentos como JSON
      const jsonParams = JSON.stringify(params);
      
      logger.info(`üêç Ejecutando: python3 ${scriptPath}`);
      
      const pythonProcess = spawn('python3', [fullScriptPath], {
        cwd: this.projectRoot,
        stdio: ['pipe', 'pipe', 'pipe']
      });
      
      let stdout = '';
      let stderr = '';
      
      // Enviar par√°metros como JSON al stdin
      pythonProcess.stdin.write(jsonParams);
      pythonProcess.stdin.end();
      
      pythonProcess.stdout.on('data', (data) => {
        stdout += data.toString();
      });
      
      pythonProcess.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      // Timeout
      const timeout = setTimeout(() => {
        pythonProcess.kill('SIGTERM');
        reject(new Error(`Timeout ejecutando ${scriptPath}`));
      }, DEFAULT_CONFIG.timeout);
      
      pythonProcess.on('close', (code) => {
        clearTimeout(timeout);
        
        if (code !== 0) {
          logger.error(`‚ùå Script Python fall√≥ con c√≥digo ${code}:`, stderr);
          reject(new Error(`Script fall√≥: ${stderr}`));
          return;
        }
        
        try {
          // Parsear respuesta JSON
          const result = JSON.parse(stdout.trim());
          resolve(result);
        } catch (parseError) {
          logger.error('‚ùå Error parseando respuesta JSON:', parseError);
          logger.error('Raw stdout:', stdout);
          reject(new Error(`Error parseando respuesta: ${parseError.message}`));
        }
      });
      
      pythonProcess.on('error', (error) => {
        clearTimeout(timeout);
        logger.error('‚ùå Error ejecutando script Python:', error);
        reject(error);
      });
    });
  }
  
  /**
   * Guarda resultado en historial de sesi√≥n
   */
  saveToSessionHistory(sessionId, result) {
    if (!this.sessionHistory.has(sessionId)) {
      this.sessionHistory.set(sessionId, []);
    }
    
    this.sessionHistory.get(sessionId).push({
      timestamp: Date.now(),
      attemptNumber: result.metadata.attemptNumber,
      modelType: result.modelType,
      score: result.metadata.langwatchTracking?.score,
      contradictionApplied: result.metadata.contradictionApplied,
      response: result.response,
      tokenUsage: result.tokenUsage,
      duration: result.metadata.duration,
      langwatch: result.metadata.langwatchTracking
    });
  }
  
  /**
   * Obtiene historial de sesi√≥n
   */
  getSessionHistory(sessionId) {
    return this.sessionHistory.get(sessionId) || [];
  }
  
  /**
   * Detecta autom√°ticamente el mejor modelo para una tarea
   */
  detectBestModel(prompt, taskType = 'general') {
    const promptLower = prompt.toLowerCase();
    
    // L√≥gica de detecci√≥n basada en contenido
    if (promptLower.includes('matem√°tica') || 
        promptLower.includes('c√°lculo') || 
        promptLower.includes('ecuaci√≥n') ||
        promptLower.includes('paradoja') ||
        taskType === 'math') {
      return 'deepseek-local';
    }
    
    if (promptLower.includes('c√≥digo') || 
        promptLower.includes('programaci√≥n') || 
        promptLower.includes('script') ||
        taskType === 'code') {
      return 'mistral-local';
    }
    
    // Por defecto, usar LLaMA para tareas generales
    return 'llama-local';
  }
  
  /**
   * Maneja fallbacks cuando un modelo no est√° disponible
   */
  async handleFallback(originalModel, prompt, options) {
    logger.warn(`‚ö†Ô∏è Modelo ${originalModel} no disponible, buscando fallback...`);
    
    const availableModels = await this.checkAvailableModels();
    const fallbackOrder = ['llama-local', 'mistral-local', 'deepseek-local'];
    
    for (const fallbackModel of fallbackOrder) {
      if (fallbackModel !== originalModel && availableModels[fallbackModel]?.modelExists) {
        logger.info(`üîÑ Usando fallback: ${fallbackModel}`);
        const result = await this.callLocalModelWithLangwatch(fallbackModel, prompt, options);
        result.fallbackUsed = true;
        result.originalModel = originalModel;
        return result;
      }
    }
    
    throw new Error('No hay modelos locales disponibles');
  }
  
  /**
   * Obtiene estad√≠sticas de Langwatch para modelos locales
   */
  getLangwatchStats() {
    return enhancedLangwatchMiddleware.getLocalModelStats();
  }
  
  /**
   * Obtiene estad√≠sticas de contradicci√≥n
   */
  getContradictionStats() {
    return contradictionService.getContradictionStats();
  }
  
  /**
   * Health check con m√©tricas de Langwatch
   */
  async healthCheckWithLangwatch() {
    try {
      const testPrompt = "Hello, this is a test. Please respond with 'OK'.";
      const results = {};
      
      for (const modelType of Object.keys(this.modelsConfig)) {
        try {
          const startTime = Date.now();
          const result = await this.callLocalModelWithLangwatch(modelType, testPrompt, {
            maxTokens: 10,
            temperature: 0.1,
            sessionId: `health_check_${Date.now()}`
          });
          
          results[modelType] = {
            status: result.success ? 'healthy' : 'error',
            responseTime: Date.now() - startTime,
            langwatchScore: result.metadata?.langwatchTracking?.score,
            error: result.error || null
          };
          
        } catch (error) {
          results[modelType] = {
            status: 'error',
            responseTime: null,
            langwatchScore: null,
            error: error.message
          };
        }
      }
      
      return {
        success: true,
        timestamp: new Date().toISOString(),
        results,
        langwatchEnabled: true,
        overallStats: this.getLangwatchStats()
      };
      
    } catch (error) {
      logger.error('‚ùå Error en health check con Langwatch:', error);
      return {
        success: false,
        error: error.message,
        timestamp: new Date().toISOString(),
        results: {},
        langwatchEnabled: false
      };
    }
  }
}

/**
 * Instancia singleton del adaptador mejorado
 */
export const enhancedLocalLLMAdapter = new EnhancedLocalLLMAdapter();

/**
 * Funci√≥n principal para llamar modelos locales con Langwatch
 */
export async function callLocalModelWithLangwatch(modelType, prompt, options = {}) {
  try {
    // Auto-detecci√≥n de modelo
    if (modelType === 'auto') {
      modelType = enhancedLocalLLMAdapter.detectBestModel(prompt, options.taskType);
      logger.info(`üéØ Auto-detectado modelo: ${modelType}`);
    }
    
    // Intentar llamada principal
    const result = await enhancedLocalLLMAdapter.callLocalModelWithLangwatch(modelType, prompt, options);
    
    if (result.success) {
      return result;
    }
    
    // Si falla, intentar fallback
    if (options.allowFallback !== false) {
      return await enhancedLocalLLMAdapter.handleFallback(modelType, prompt, options);
    }
    
    return result;
    
  } catch (error) {
    logger.error('‚ùå Error en callLocalModelWithLangwatch:', error);
    throw error;
  }
}

/**
 * Funci√≥n para obtener informaci√≥n de modelos disponibles con Langwatch
 */
export async function getAvailableLocalModelsWithLangwatch() {
  try {
    if (!enhancedLocalLLMAdapter.isInitialized) {
      await enhancedLocalLLMAdapter.initialize();
    }
    
    const available = await enhancedLocalLLMAdapter.checkAvailableModels();
    const modelsInfo = Object.entries(ENHANCED_LOCAL_MODELS_CONFIG).map(([type, config]) => ({
      type,
      displayName: config.displayName,
      description: config.description,
      maxTokens: config.maxTokens,
      temperature: config.temperature,
      modelPath: config.modelPath,
      scriptPath: config.scriptPath,
      langwatch: config.langwatch
    }));
    
    return {
      success: true,
      models: modelsInfo,
      availability: available,
      totalConfigured: modelsInfo.length,
      totalAvailable: Object.values(available).filter(m => m.modelExists && m.scriptExists).length,
      langwatchEnabled: true,
      stats: enhancedLocalLLMAdapter.getLangwatchStats()
    };
    
  } catch (error) {
    logger.error('‚ùå Error obteniendo modelos disponibles con Langwatch:', error);
    return {
      success: false,
      error: error.message,
      models: [],
      availability: {},
      totalConfigured: 0,
      totalAvailable: 0,
      langwatchEnabled: false
    };
  }
}

/**
 * Funci√≥n para health check con Langwatch
 */
export async function healthCheckLocalModelsWithLangwatch() {
  return await enhancedLocalLLMAdapter.healthCheckWithLangwatch();
}

export default {
  EnhancedLocalLLMAdapter,
  enhancedLocalLLMAdapter,
  callLocalModelWithLangwatch,
  getAvailableLocalModelsWithLangwatch,
  healthCheckLocalModelsWithLangwatch,
  ENHANCED_LOCAL_MODELS_CONFIG
};

